# Resource URLs for This Workshop

---

## Review of the OWASP Top 10 for LLM Applications

#### OWASP Top 10 for Large Language Model Applications
https://owasp.org/www-project-top-10-for-large-language-model-applications/

#### OWASP Top 10 for Large Language Model Applications PDF, Version 1.0.1
https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v1_0_1.pdf

#### OWASP Main Site
https://owasp.org/

---

## Supply Chain Security & Model Security (Data Poisoning and Malicious Models)

#### Arbitrary Code Execution in langchain | CVE-2023-29374 | Snyk
https://security.snyk.io/vuln/SNYK-PYTHON-LANGCHAIN-5411357

#### PoisonGPT: How to poison LLM supply chainon Hugging Face (mithrilsecurity.io)
https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news/

#### Hugging Face – The AI community building the future.
https://huggingface.co/

---

## LLM Plugins Security and Excessive Agency

#### Plugin Vulnerabilities: Visit a Website and Have Your Source Code Stolen · Embrace The Red
https://embracethered.com/blog/posts/2023/chatgpt-plugin-vulns-chat-with-code/

#### ChatGPT Plugin Exploit Explained: From Prompt Injection to Accessing Private Data · Embrace The Red
https://embracethered.com/blog/posts/2023/chatgpt-cross-plugin-request-forgery-and-prompt-injection./

#### ChatGPT Plugin Exploit Explained: From Prompt Injection to Accessing Private Data · Embrace The Red
https://embracethered.com/blog/posts/2023/chatgpt-cross-plugin-request-forgery-and-prompt-injection./

---

## Hands-on lab setup 

#### Python.org
https://www.python.org/

#### Visual Studio Code - Code Editing. Redefined
https://code.visualstudio.com/

#### GitHub Desktop | Simple collaboration from your desktop
https://desktop.github.com/

#### Git Guides - install git (github.com)
https://github.com/git-guides/install-git

---

## Prompt Injection and “Jailbreaking” Security for LLM Applications 

#### AI Injections: Direct and Indirect Prompt Injections and Their Implications · Embrace The Red
https://embracethered.com/blog/posts/2023/ai-injections-direct-and-indirect-prompt-injection-basics/

#### Prompt injection attack on ChatGPT steals chat data | System Weakness
https://systemweakness.com/new-prompt-injection-attack-on-chatgpt-web-version-ef717492c5c2

---

## Implementing prompt and output validation using NVidea NeMo Guardrails 

#### NVIDIA/NeMo-Guardrails: NeMo Guardrails is an open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems. (github.com)
https://github.com/NVIDIA/NeMo-Guardrails

#### NeMo-Guardrails/docs/getting_started/hello-world.md at main · NVIDIA/NeMo-Guardrails (github.com)
https://github.com/NVIDIA/NeMo-Guardrails/blob/main/docs/getting_started/hello-world.md
